{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ayushi2001/Similarity-Comparison/blob/main/Copy_of_loan_default_pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "98cc6a87",
      "metadata": {
        "id": "98cc6a87"
      },
      "source": [
        "\n",
        "# Loan Default Prediction — Modular ML Pipeline (Notebook Template)\n",
        "\n",
        "This notebook is **data-agnostic** and modular. Set the path to your dataset and run all cells.\n",
        "\n",
        "**How to use**\n",
        "1. Put your dataset file at `/mnt/data/Dataset.csv` (or change `DATA_PATH` below).\n",
        "2. (Optional) Put `Data_Dictionary.csv` next to it for quick reference.\n",
        "3. Run cells top-to-bottom. The pipeline will:\n",
        "   - Auto-detect the target column (with sensible fallbacks)\n",
        "   - Build preprocessing and modeling pipelines\n",
        "   - Handle class imbalance (class weights; optional SMOTE if available)\n",
        "   - Evaluate and tune threshold\n",
        "   - Log experiments to a CSV\n",
        "   - Save artifacts (model + metadata)\n",
        "   - Run basic unit tests\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bea33688",
      "metadata": {
        "id": "bea33688"
      },
      "outputs": [],
      "source": [
        "\n",
        "# === 0) Configuration ===\n",
        "DATA_PATH = \"/mnt/data/Dataset.csv\"      # <-- change if needed\n",
        "DATA_DICTIONARY_PATH = \"/mnt/data/Data_Dictionary.csv\"\n",
        "TARGET_HINT = None   # e.g., \"default\" if you know the exact target column name\n",
        "ARTIFACT_DIR = \"/mnt/data/artifacts\"\n",
        "EXP_LOG = \"/mnt/data/experiments.csv\"\n",
        "\n",
        "import os, json, math, warnings, joblib, numpy as np, pandas as pd\n",
        "from pathlib import Path\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "Path(ARTIFACT_DIR).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"Configured paths:\")\n",
        "print(\"DATA_PATH:\", DATA_PATH)\n",
        "print(\"DATA_DICTIONARY_PATH:\", DATA_DICTIONARY_PATH)\n",
        "print(\"ARTIFACT_DIR:\", ARTIFACT_DIR)\n",
        "print(\"EXP_LOG:\", EXP_LOG)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80330131",
      "metadata": {
        "id": "80330131"
      },
      "outputs": [],
      "source": [
        "\n",
        "# === 1) Imports ===\n",
        "from datetime import datetime\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import (roc_auc_score, precision_recall_curve, roc_curve,\n",
        "                             classification_report, confusion_matrix, f1_score, precision_score, recall_score)\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Optional libs\n",
        "try:\n",
        "    from xgboost import XGBClassifier\n",
        "    HAS_XGB = True\n",
        "except Exception:\n",
        "    HAS_XGB = False\n",
        "\n",
        "try:\n",
        "    from imblearn.over_sampling import SMOTE\n",
        "    from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "    HAS_IMB = True\n",
        "except Exception:\n",
        "    HAS_IMB = False\n",
        "\n",
        "print(\"Environment — XGB:\", HAS_XGB, \"| imblearn:\", HAS_IMB)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c86f8b4f",
      "metadata": {
        "id": "c86f8b4f"
      },
      "outputs": [],
      "source": [
        "\n",
        "# === 2) Load Data ===\n",
        "import pandas as pd, numpy as np\n",
        "\n",
        "df = pd.read_csv(DATA_PATH)\n",
        "print(\"Shape:\", df.shape)\n",
        "display(df.head())\n",
        "\n",
        "if os.path.exists(DATA_DICTIONARY_PATH):\n",
        "    try:\n",
        "        dd = pd.read_csv(DATA_DICTIONARY_PATH)\n",
        "        print(\"\\nData dictionary (first rows):\")\n",
        "        display(dd.head())\n",
        "    except Exception as e:\n",
        "        print(\"Could not read data dictionary:\", e)\n",
        "\n",
        "# Target detection\n",
        "possible_targets = [\"default\",\"Default\",\"loan_default\",\"Loan_Default\",\"is_default\",\"target\",\"TARGET\",\"label\"]\n",
        "target_col = TARGET_HINT if TARGET_HINT in df.columns else None\n",
        "if target_col is None:\n",
        "    for c in df.columns:\n",
        "        if c in possible_targets:\n",
        "            target_col = c\n",
        "            break\n",
        "if target_col is None:\n",
        "    for c in df.columns:\n",
        "        unique_vals = df[c].dropna().unique()\n",
        "        if len(unique_vals) <= 3 and set(map(str, unique_vals)).issubset(set(map(str, [0,1,\"0\",\"1\",\"Y\",\"N\",\"y\",\"n\",\"Yes\",\"No\"]))):\n",
        "            if \"id\" not in c.lower() and \"date\" not in c.lower():\n",
        "                target_col = c\n",
        "                break\n",
        "\n",
        "assert target_col is not None, \"❌ Could not auto-detect the target column. Set TARGET_HINT to the correct column name.\"\n",
        "\n",
        "def normalize_binary(s):\n",
        "    mapping = {\"Y\":1,\"y\":1,\"Yes\":1,\"YES\":1,\"N\":0,\"n\":0,\"No\":0,\"NO\":0,\"1\":1,\"0\":0}\n",
        "    return s.map(lambda x: mapping.get(str(x), x)).astype(float)\n",
        "\n",
        "uniq = df[target_col].dropna().unique()\n",
        "if not set(np.unique(np.array(list(map(str, uniq))))).issubset(set(map(str, [0,1]))):\n",
        "    df[target_col] = normalize_binary(df[target_col]).astype(int)\n",
        "else:\n",
        "    df[target_col] = df[target_col].astype(int)\n",
        "\n",
        "print(\"Detected target:\", target_col)\n",
        "print(\"Target counts:\\n\", df[target_col].value_counts(dropna=False))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "653c69d2",
      "metadata": {
        "id": "653c69d2"
      },
      "outputs": [],
      "source": [
        "\n",
        "# === 3) Quick EDA ===\n",
        "print(\"Missing values per column (top 20):\")\n",
        "display(df.isna().sum().sort_values(ascending=False).head(20))\n",
        "\n",
        "pos_rate = df[target_col].mean()\n",
        "print(f\"Positive rate: {pos_rate:.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "949b281d",
      "metadata": {
        "id": "949b281d"
      },
      "outputs": [],
      "source": [
        "\n",
        "# === 4) Feature Columns & Types ===\n",
        "id_like = [c for c in df.columns if any(k in c.lower() for k in [\"id\",\"uuid\",\"guid\"])]\n",
        "feature_cols = [c for c in df.columns if c not in [target_col] + id_like]\n",
        "\n",
        "X_raw = df[feature_cols].copy()\n",
        "y = df[target_col].astype(int).values\n",
        "\n",
        "num_cols = X_raw.select_dtypes(include=[\"number\"]).columns.tolist()\n",
        "cat_cols = X_raw.select_dtypes(exclude=[\"number\"]).columns.tolist()\n",
        "print(\"Numeric:\", len(num_cols), \"| Categorical:\", len(cat_cols))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f80de32",
      "metadata": {
        "id": "4f80de32"
      },
      "outputs": [],
      "source": [
        "\n",
        "# === 5) Conditional Feature Engineering ===\n",
        "import numpy as np\n",
        "def add_engineered_features(df_in):\n",
        "    df_out = df_in.copy()\n",
        "    # Create ratios only if sources exist\n",
        "    ratios = [\n",
        "        (\"loan_to_income\", \"loan_amount\", \"annual_income\"),\n",
        "        (\"debt_to_income\", \"total_debt\", \"annual_income\"),\n",
        "        (\"emi_to_income\", \"monthly_installment\", \"monthly_income\"),\n",
        "        (\"credit_utilization_ratio\", \"revolving_balance\", \"revolving_limit\"),\n",
        "    ]\n",
        "    for new_name, num, denom in ratios:\n",
        "        if num in df_out.columns and denom in df_out.columns:\n",
        "            with np.errstate(divide='ignore', invalid='ignore'):\n",
        "                df_out[new_name] = (df_out[num].astype(float) /\n",
        "                                    df_out[denom].replace(0, np.nan).astype(float)).fillna(0.0)\n",
        "    return df_out\n",
        "\n",
        "X = add_engineered_features(X_raw)\n",
        "num_cols = X.select_dtypes(include=[\"number\"]).columns.tolist()\n",
        "cat_cols = X.select_dtypes(exclude=[\"number\"]).columns.tolist()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d55ab39f",
      "metadata": {
        "id": "d55ab39f"
      },
      "outputs": [],
      "source": [
        "\n",
        "# === 6) Preprocessing Pipeline ===\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "    (\"scaler\", StandardScaler())\n",
        "])\n",
        "\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "    (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))\n",
        "])\n",
        "\n",
        "preprocess = ColumnTransformer(transformers=[\n",
        "    (\"num\", numeric_transformer, num_cols),\n",
        "    (\"cat\", categorical_transformer, cat_cols)\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb749acd",
      "metadata": {
        "id": "bb749acd"
      },
      "outputs": [],
      "source": [
        "\n",
        "# === 7) Train/Test Split & Model Candidates ===\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "def make_pipeline(clf, use_smote=False):\n",
        "    try:\n",
        "        if use_smote and HAS_IMB:\n",
        "            from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "            from imblearn.over_sampling import SMOTE\n",
        "            return ImbPipeline([\n",
        "                (\"preprocess\", preprocess),\n",
        "                (\"smote\", SMOTE(random_state=42)),\n",
        "                (\"clf\", clf),\n",
        "            ])\n",
        "    except Exception:\n",
        "        pass\n",
        "    return Pipeline([(\"preprocess\", preprocess), (\"clf\", clf)])\n",
        "\n",
        "candidates = [\n",
        "    (\"LogReg_weighted\",\n",
        "     make_pipeline(LogisticRegression(max_iter=200, class_weight=\"balanced\"))),\n",
        "    (\"RandomForest\",\n",
        "     make_pipeline(RandomForestClassifier(n_estimators=300, class_weight=\"balanced_subsample\", n_jobs=-1, random_state=42)))\n",
        "]\n",
        "\n",
        "try:\n",
        "    if HAS_XGB:\n",
        "        candidates.append((\"XGBClassifier\", make_pipeline(\n",
        "            XGBClassifier(\n",
        "                n_estimators=500, max_depth=5, learning_rate=0.05,\n",
        "                subsample=0.8, colsample_bytree=0.8, objective=\"binary:logistic\",\n",
        "                eval_metric=\"logloss\", random_state=42\n",
        "            ))))\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "if HAS_IMB:\n",
        "    candidates.append((\"LogReg_SMOTE\", make_pipeline(LogisticRegression(max_iter=200), use_smote=True)))\n",
        "\n",
        "results = []\n",
        "best_auc, best_model, best_name = -1, None, None\n",
        "for name, pipe in candidates:\n",
        "    pipe.fit(X_train, y_train)\n",
        "    proba = pipe.predict_proba(X_test)[:,1]\n",
        "    auc = roc_auc_score(y_test, proba)\n",
        "    preds = (proba >= 0.5).astype(int)\n",
        "    f1 = f1_score(y_test, preds)\n",
        "    pr = precision_score(y_test, preds)\n",
        "    re = recall_score(y_test, preds)\n",
        "    results.append((name, auc, f1, pr, re))\n",
        "    print(f\"{name:16s} | AUC: {auc:.4f} | F1: {f1:.4f} | P: {pr:.4f} | R: {re:.4f}\")\n",
        "    if auc > best_auc:\n",
        "        best_auc, best_model, best_name = auc, pipe, name\n",
        "\n",
        "import pandas as pd\n",
        "res_df = pd.DataFrame(results, columns=[\"model\",\"auc\",\"f1\",\"precision\",\"recall\"]).sort_values(\"auc\", ascending=False)\n",
        "print(\"\\nBest model:\", best_name, \"AUC:\", best_auc)\n",
        "display(res_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "095db600",
      "metadata": {
        "id": "095db600"
      },
      "outputs": [],
      "source": [
        "\n",
        "# === 8) Curves & Threshold Tuning ===\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_curve, precision_recall_curve, classification_report, confusion_matrix\n",
        "\n",
        "proba_best = best_model.predict_proba(X_test)[:,1]\n",
        "\n",
        "fpr, tpr, thr = roc_curve(y_test, proba_best)\n",
        "plt.figure()\n",
        "plt.plot(fpr, tpr, label=f\"ROC AUC = {roc_auc_score(y_test, proba_best):.3f}\")\n",
        "plt.plot([0,1],[0,1],'--')\n",
        "plt.xlabel(\"FPR\"); plt.ylabel(\"TPR\"); plt.title(\"ROC\"); plt.legend(); plt.show()\n",
        "\n",
        "prec, rec, pr_thr = precision_recall_curve(y_test, proba_best)\n",
        "plt.figure()\n",
        "plt.plot(rec, prec)\n",
        "plt.xlabel(\"Recall\"); plt.ylabel(\"Precision\"); plt.title(\"PR Curve\"); plt.show()\n",
        "\n",
        "f1_scores = [(t, f1_score(y_test, (proba_best >= t).astype(int))) for t in np.linspace(0.1, 0.9, 81)]\n",
        "best_t, best_f1 = max(f1_scores, key=lambda x: x[1])\n",
        "print(f\"Best threshold by F1: {best_t:.3f} (F1={best_f1:.3f})\")\n",
        "\n",
        "preds_opt = (proba_best >= best_t).astype(int)\n",
        "print(\"\\nClassification Report (best threshold)\")\n",
        "print(classification_report(y_test, preds_opt, digits=4))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, preds_opt))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2219fd93",
      "metadata": {
        "id": "2219fd93"
      },
      "outputs": [],
      "source": [
        "\n",
        "# === 9) Log Experiment & Save Artifacts ===\n",
        "from datetime import datetime\n",
        "import pandas as pd, json, joblib, os\n",
        "\n",
        "exp_record = {\n",
        "    \"timestamp\": datetime.utcnow().isoformat(),\n",
        "    \"best_model\": best_name,\n",
        "    \"best_auc\": float(best_auc),\n",
        "    \"threshold\": float(best_t),\n",
        "    \"train_rows\": int(len(X_train)),\n",
        "    \"test_rows\": int(len(X_test)),\n",
        "    \"num_features\": len(X.select_dtypes(include=['number']).columns),\n",
        "    \"cat_features\": len(X.select_dtypes(exclude=['number']).columns),\n",
        "}\n",
        "\n",
        "if os.path.exists(EXP_LOG):\n",
        "    old = pd.read_csv(EXP_LOG)\n",
        "    out = pd.concat([old, pd.DataFrame([exp_record])], ignore_index=True)\n",
        "else:\n",
        "    out = pd.DataFrame([exp_record])\n",
        "out.to_csv(EXP_LOG, index=False)\n",
        "print(\"Logged experiment to:\", EXP_LOG)\n",
        "display(out.tail(5))\n",
        "\n",
        "# Save model + metadata\n",
        "model_path = os.path.join(ARTIFACT_DIR, f\"model_{best_name}.joblib\")\n",
        "joblib.dump(best_model, model_path)\n",
        "\n",
        "meta = {\n",
        "    \"target_col\": target_col,\n",
        "    \"best_model\": best_name,\n",
        "    \"threshold\": float(best_t),\n",
        "}\n",
        "with open(os.path.join(ARTIFACT_DIR, \"metadata.json\"), \"w\") as f:\n",
        "    json.dump(meta, f, indent=2)\n",
        "\n",
        "print(\"Saved model:\", model_path)\n",
        "print(\"Saved metadata:\", os.path.join(ARTIFACT_DIR, \"metadata.json\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e14e10f9",
      "metadata": {
        "id": "e14e10f9"
      },
      "outputs": [],
      "source": [
        "\n",
        "# === 10) Inference Helper ===\n",
        "import numpy as np, pandas as pd\n",
        "\n",
        "def predict_default(df_raw: pd.DataFrame, threshold=None):\n",
        "    proba = best_model.predict_proba(df_raw)[:,1]\n",
        "    t = threshold if threshold is not None else best_t\n",
        "    return pd.DataFrame({\"proba\": proba, \"pred\": (proba >= t).astype(int)})\n",
        "\n",
        "# Demo on 5 samples\n",
        "demo = X.iloc[:5].copy()\n",
        "display(demo.join(predict_default(demo)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "556ebbe3",
      "metadata": {
        "id": "556ebbe3"
      },
      "outputs": [],
      "source": [
        "\n",
        "# === 11) Basic Unit Tests ===\n",
        "\n",
        "def test_no_nulls_after_preprocess():\n",
        "    Z = best_model.named_steps.get(\"preprocess\").fit_transform(X, y)\n",
        "    import numpy as np\n",
        "    assert not np.isnan(Z).any(), \"NaNs found after preprocess\"\n",
        "\n",
        "def test_probability_bounds():\n",
        "    import numpy as np\n",
        "    p = best_model.predict_proba(X.iloc[:20])[:,1]\n",
        "    assert np.all((p>=0)&(p<=1)), \"Probabilities out of range\"\n",
        "\n",
        "def test_inference_shape():\n",
        "    preds = predict_default(X.iloc[:10])\n",
        "    assert len(preds)==10 and {\"proba\",\"pred\"}.issubset(preds.columns), \"Inference output malformed\"\n",
        "\n",
        "for fn in [test_no_nulls_after_preprocess, test_probability_bounds, test_inference_shape]:\n",
        "    fn()\n",
        "print(\"✅ Basic unit tests passed.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24944208",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "24944208",
        "outputId": "ae385230-4048-4b86-dee5-5d5b9658382d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python: 3.12.11 (main, Jun  4 2025, 08:56:18) [GCC 11.4.0]\n",
            "pandas: 2.2.2\n",
            "numpy: 2.0.2\n",
            "sklearn: 1.6.1\n",
            "xgboost: 3.0.4\n",
            "imblearn: 0.14.0\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# === 12) Environment Versions ===\n",
        "import sklearn, sys, pandas as pd, numpy as np\n",
        "print(\"Python:\", sys.version)\n",
        "print(\"pandas:\", pd.__version__)\n",
        "print(\"numpy:\", np.__version__)\n",
        "print(\"sklearn:\", sklearn.__version__)\n",
        "try:\n",
        "    import xgboost as xgb\n",
        "    print(\"xgboost:\", xgb.__version__)\n",
        "except Exception:\n",
        "    print(\"xgboost: not available\")\n",
        "try:\n",
        "    import imblearn\n",
        "    print(\"imblearn:\", imblearn.__version__)\n",
        "except Exception:\n",
        "    print(\"imblearn: not available\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3zFmjhqenzM3"
      },
      "id": "3zFmjhqenzM3",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}